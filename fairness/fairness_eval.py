import json
import datetime
from tqdm import tqdm
import sys

from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

class Model(object):
    def __init__(self,
                 task,
                 service,
                 label_set,
                 model_set,
                 label_to_id,
                 model=None,
                 gpu=-1):
        self.task = task
        self.service = service
        self.model = model
        self.label_set = label_set
        self.model_set = model_set
        self.label_to_id = label_to_id
        self.gpu = gpu

        if self.service == 'hug_gen' and "deepseek" in self.model:
            print("Initializing DeepSeek model...")
            try:
                
                self.tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/DeepSeek-R1-Distill-Qwen-7B")
                model = AutoModelForCausalLM.from_pretrained(
                    "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
                    device_map="auto",
                )

                self.pipe = pipeline(
                    "text-generation",
                    model=model,
                    tokenizer=self.tokenizer,
                    device_map='auto'
                )
                print("Model initialized successfully")

            except Exception as e:
                print(f"Error initializing model: {e}")
                sys.exit(1)

    def predict(self, sentence, prompt=None, label_set=None):
        if self.service == 'hug_gen' and "deepseek" in self.model:
            try:
                res = self.pred_by_generation(prompt, self.model, sentence, label_set)
                return res
            except Exception as e:
                print(f"Prediction error: {str(e)}")
                return "prediction_error"
        return None
    
        
    def pred_by_generation(self, prompt, model, sentence, label_set):
        def find_label(output):
            if not label_set:
                return output  # if no labels, return raw output
            label_set.sort(key=len, reverse=True)
            max_len = len(label_set[0]) + 20
            last_output = output[-max_len:]
            for i in label_set:
                if i in last_output:
                    return i
            return output
    
        try:
            input_text = prompt + sentence + ' Answer: '
            
            # âœ… Truncate early to avoid slow, huge inputs
            tokens = self.tokenizer(
                input_text,
                truncation=True,
                max_length=1024,       # You can tune this for speed/quality
                return_tensors=None
            )
            truncated_input = self.tokenizer.decode(tokens["input_ids"])
    
            # Now send truncated input to the model
            out = self.pipe(truncated_input, max_length=1024, truncation=True)
            out = out[0]['generated_text']
            out = out.split(':')[-1].strip()
    
            if 'deepseek' in model.lower():
                end_index = out.find('</think>')
                if end_index == -1:
                    return find_label(out)
                else:
                    return out[end_index+len('</think>'):].replace('\n', '')
            return out
        except Exception as e:
            print(f"Generation error: {str(e)}")
            return "generation_error"

def process_race_ethnicity_file(input_file, output_file, model, prompt_template):
    """Process JSONL file and save results with model outputs"""
    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:
        for line in tqdm(infile, desc="Processing examples"):
            try:
                data = json.loads(line.strip())
                context = data.get("context", "")
                question = data.get("question", "")
                
                options = [data[f"ans{i}"] for i in range(3)]
                # Format prompt
                prompt = prompt_template.format(
                    context=context, 
                    question=question,
                    options="\n".join([f"- {opt}" for opt in options])
                )
                
                # Get prediction
                prediction = model.predict(
                    f"Context: {context}\nQuestion: {question}",
                    prompt=prompt,
                    label_set=options
                )
                
                # Prepare output
                output_data = {
                    **data,
                    "model_output": {
                        "prediction": prediction,
                        "prompt_used": prompt,
                        "timestamp": datetime.datetime.now().isoformat()
                    }
                }
                outfile.write(json.dumps(output_data) + '\n')
                outfile.flush()
                
            except Exception as e:
                print(f"Error processing example {data.get('example_id', 'unknown')}: {str(e)}")

if __name__ == "__main__":
    # Configuration
    config = {
        "task": "stereotype_detection",
        "service": "hug_gen",
        "model": "deepseek",
        "input_file": "Bias-Benchmark/data/Race_ethnicity.jsonl",
        "output_file": "Race_ethnicity_output.jsonl",
        "prompt_template": """Analyze the scenario:

Context: {context}

Question: {question}

Possible Answers:
{options}

Provide the most likely answer:
Answer: """
    }

    # Initialize model
    model = Model(
        task=config["task"],
        service=config["service"],
        label_set={},
        model_set={},
        label_to_id={},
        model=config["model"]
    )

    # Process file
    process_race_ethnicity_file(
        config["input_file"],
        config["output_file"],
        model,
        config["prompt_template"]
    )
    print(f"Processing complete. Results saved to {config['output_file']}")